window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "micro_sam", "modulename": "micro_sam", "kind": "module", "doc": "<h1 id=\"segment-anything-for-microscopy\">Segment Anything for Microscopy</h1>\n\n<p>Segment Anything for Microscopy implements automatic and interactive annotation for microscopy data. It is built on top of <a href=\"https://segment-anything.com/\">Segment Anything</a> by Meta AI and specializes it for microscopy and other bio-imaging data.\nIts core components are:</p>\n\n<ul>\n<li>The <code>micro_sam</code> annotator tools for interactive data annotation with <a href=\"https://napari.org/stable/\">napari</a>.</li>\n<li>The <code>micro_sam</code> library to apply Segment Anything to 2d and 3d data or fine-tune it on your data.</li>\n<li>The <code>micro_sam</code> models that are fine-tuned on publicly available microscopy data.</li>\n</ul>\n\n<p>Our goal is to build fast and interactive annotation tools for microscopy data, like interactive cell segmentation from bounding boxes:</p>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d04cb158-9f5b-4460-98cd-023c4f19cccd\" alt=\"box-prompts\" /></p>\n\n<p><code>micro_sam</code> is under active development, but our goal is to keep the changes to the user interface and the interface of the python library as small as possible.\nOn our roadmap for more functionality are:</p>\n\n<ul>\n<li>Providing an installer for running <code>micro_sam</code> as a standalone application.</li>\n<li>Releasing more and better finetuned models as well as the code for fine-tuning.</li>\n<li>Integration of the finetuned models with <a href=\"https://bioimage.io/#/\">bioimage.io</a></li>\n<li>Implementing a napari plugin for <code>micro_sam</code>.</li>\n</ul>\n\n<p>If you run into any problems or have questions please open an issue on Github or reach out via <a href=\"https://forum.image.sc/\">image.sc</a> using the tag <code>micro-sam</code> and tagging @constantinpape.</p>\n\n<!----\nBetter instance segmentation, Few-shot adapation (using LORA, QLORA, etc.)\n---->\n\n<h2 id=\"quickstart\">Quickstart</h2>\n\n<p>You can install <code>micro_sam</code> via conda:</p>\n\n<pre><code>$ conda install -c conda-forge micro_sam\n</code></pre>\n\n<p>For more installation options check out <a href=\"#installation\">Installation</a></p>\n\n<p>After installing <code>micro_sam</code> you can run the annotation tool via <code>$ micro_sam.annotator</code>, which opens a menu for selecting the annotation tool and its inputs.\nSee <a href=\"#annotation-tools\">Annotation Tools</a> for an overview and explanation of the annotation functionality.</p>\n\n<p>The <code>micro_sam</code> python library can be used via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam</span>\n</code></pre>\n</div>\n\n<p>It is explained in more detail <a href=\"#how-to-use-the-python-library\">here</a>.</p>\n\n<p>Our support for finetuned models is still experimental. We will soon release better finetuned models and host them on zenodo.\nFor now, check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/sam_annotator_2d.py#L62\">the example script for the 2d annotator</a> to see how the finetuned models can be used within <code>micro_sam</code>.</p>\n\n<h2 id=\"citation\">Citation</h2>\n\n<p>If you are using <code>micro_sam</code> in your research please cite</p>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/2304.02643\">SegmentAnything</a></li>\n<li>and our repository on <a href=\"https://doi.org/10.5281/zenodo.7919746\">zenodo</a></li>\n</ul>\n\n<p>We will release a preprint soon that describes this work and can be cited instead of zenodo.</p>\n\n<h1 id=\"installation\">Installation</h1>\n\n<p><code>micro_sam</code> requires the following dependencies:</p>\n\n<ul>\n<li><a href=\"https://pytorch.org/get-started/locally/\">PyTorch</a></li>\n<li><a href=\"https://github.com/facebookresearch/segment-anything#installation\">SegmentAnything</a></li>\n<li><a href=\"https://napari.org/stable/\">napari</a></li>\n<li><a href=\"https://github.com/constantinpape/elf\">elf</a></li>\n</ul>\n\n<p>It is available as a conda package and can be installed via</p>\n\n<pre><code>$ conda install -c conda-forge micro_sam\n</code></pre>\n\n<h2 id=\"from-source\">From source</h2>\n\n<p>To install <code>micro_sam</code> from source, we recommend to first set up a conda environment with the necessary requirements:</p>\n\n<ul>\n<li><a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_gpu.yaml\">environment_gpu.yaml</a>: sets up an environment with GPU support.</li>\n<li><a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_cpu.yaml\">environment_cpu.yaml</a>: sets up an environment with CPU support.</li>\n</ul>\n\n<p>To create one of these environments and install <code>micro_sam</code> into it follow these steps</p>\n\n<ol>\n<li>Clone the repository:</li>\n</ol>\n\n<pre><code>$ git clone https://github.com/computational-cell-analytics/micro-sam\n</code></pre>\n\n<ol start=\"2\">\n<li>Enter it:</li>\n</ol>\n\n<pre><code>$ cd micro_sam\n</code></pre>\n\n<ol start=\"3\">\n<li>Create the GPU or CPU environment:</li>\n</ol>\n\n<pre><code>$ conda env create -f &lt;ENV_FILE&gt;.yaml\n</code></pre>\n\n<ol start=\"4\">\n<li>Activate the environment:</li>\n</ol>\n\n<pre><code>$ conda activate sam\n</code></pre>\n\n<ol start=\"5\">\n<li>Install <code>micro_sam</code>:</li>\n</ol>\n\n<pre><code>$ pip install -e .\n</code></pre>\n\n<p><strong>Troubleshooting:</strong></p>\n\n<ul>\n<li>On some systems <code>conda</code> is extremely slow and cannot resolve the environment in the step <code>conda env create ...</code>. You can use <code>mamba</code> instead, which is a faster re-implementation of <code>conda</code>. It can resolve the environment in less than a minute on any system we tried. Check out <a href=\"https://mamba.readthedocs.io/en/latest/installation.html\">this link</a> for how to install <code>mamba</code>. Once you have installed it, run <code>mamba env create -f &lt;ENV_FILE&gt;.yaml</code> to create the env.</li>\n<li>Installation on MAC with a M1 or M2 processor:\n<ul>\n<li>The pytorch installation from <code>environment_cpu.yaml</code> does not work with a MAC that has an M1 or M2 processor. Instead you need to:\n<ul>\n<li>Create a new environment: <code>mamba create -c conda-forge python pip -n sam</code></li>\n<li>Activate it va <code>mamba activate sam</code></li>\n<li>Follow the instructions for how to install pytorch for MAC via conda from <a href=\"https://pytorch.org/\">pytorch.org</a>.</li>\n<li>Install additional dependencies: <code>mamba install -c conda-forge napari python-elf tqdm</code></li>\n<li>Install SegmentAnything: <code>pip install git+https://github.com/facebookresearch/segment-anything.git</code></li>\n<li>Install <code>micro_sam</code> by running <code>pip install -e .</code> in this folder.</li>\n</ul></li>\n<li><strong>Note:</strong> we have seen many issues with the pytorch installation on MAC. If a wrong pytorch version is installed for you (which will cause pytorch errors once you run the application) please try again with a clean <code>mambaforge</code> installation. Please install the <code>OS X, arm64</code> version from <a href=\"https://github.com/conda-forge/miniforge#mambaforge\">here</a>.</li>\n<li>Some MACs require a specific installation order of packages. If the steps layed out above don't work for you please check out the procedure described <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/77\">in this github issue</a>.</li>\n</ul></li>\n</ul>\n\n<h1 id=\"annotation-tools\">Annotation Tools</h1>\n\n<p><code>micro_sam</code> provides applications for fast interactive 2d segmentation, 3d segmentation and tracking.\nSee an example for interactive cell segmentation in phase-contrast microscopy (left), interactive segmentation\nof mitochondria in volume EM (middle) and interactive tracking of cells (right).</p>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d5ee2080-ab08-4716-b4c4-c169b4ed29f5\" width=\"256\">\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/dfca3d9b-dba5-440b-b0f9-72a0683ac410\" width=\"256\">\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/aefbf99f-e73a-4125-bb49-2e6592367a64\" width=\"256\"></p>\n\n<p>The annotation tools can be started from the <code>micro_sam</code> GUI, the command line or from python scripts. The <code>micro_sam</code> GUI can be started by</p>\n\n<pre><code>$ micro_sam.annotator\n</code></pre>\n\n<p>They are built with <a href=\"https://napari.org/stable/\">napari</a> to implement the viewer and user interaction.\nIf you are not familiar with napari yet, <a href=\"https://napari.org/stable/tutorials/fundamentals/quick_start.html\">start here</a>.\nThe <code>micro_sam</code> applications are mainly based on <a href=\"https://napari.org/stable/howtos/layers/points.html\">the point layer</a>, <a href=\"https://napari.org/stable/howtos/layers/shapes.html\">the shape layer</a> and <a href=\"https://napari.org/stable/howtos/layers/labels.html\">the label layer</a>.</p>\n\n<h2 id=\"annotator-2d\">Annotator 2D</h2>\n\n<p>The 2d annotator can be started by</p>\n\n<ul>\n<li>clicking <code>2d annotator</code> in the <code>micro_sam</code> GUI.</li>\n<li>running <code>$ micro_sam.annotator_2d</code> in the command line. Run <code>micro_sam.annotator_2d -h</code> for details.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_2d</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py\">examples/annotator_2d.py</a> for details. </li>\n</ul>\n\n<p>The user interface of the 2d annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/2d-annotator-menu.png\" width=\"768\"></p>\n\n<p>It contains the following elements:</p>\n\n<ol>\n<li>The napari layers for the image, segmentations and prompts:\n<ul>\n<li><code>box_prompts</code>: shape layer that is used to provide box prompts to SegmentAnything.</li>\n<li><code>prompts</code>: point layer that is used to provide prompts to SegmentAnything. Positive prompts (green points) for marking the object you want to segment, negative prompts (red points) for marking the outside of the object.</li>\n<li><code>current_object</code>: label layer that contains the object you're currently segmenting.</li>\n<li><code>committed_objects</code>: label layer with the objects that have already been segmented.</li>\n<li><code>auto_segmentation</code>: label layer results from using SegmentAnything for automatic instance segmentation.</li>\n<li><code>raw</code>: image layer that shows the image data.</li>\n</ul></li>\n<li>The prompt menu for changing the currently selected point from positive to negative and vice versa. This can also be done by pressing <code>t</code>.</li>\n<li>The menu for automatic segmentation. Pressing <code>Segment All Objects</code> will run automatic segmentation. The results will be displayed in the <code>auto_segmentation</code> layer. Change the parameters <code>pred iou thresh</code> and <code>stability score thresh</code> to control how many objects are segmented.</li>\n<li>The menu for interactive segmentation. Pressing <code>Segment Object</code> (or <code>s</code>) will run segmentation for the current prompts. The result is displayed in <code>current_object</code></li>\n<li>The menu for commiting the segmentation. When pressing <code>Commit</code> (or <code>c</code>) the result from the selected layer (either <code>current_object</code> or <code>auto_segmentation</code>) will be transferred from the respective layer to <code>committed_objects</code>.</li>\n<li>The menu for clearing the current annotations. Pressing <code>Clear Annotations</code> (or <code>shift c</code>) will clear the current annotations and the current segmentation.</li>\n</ol>\n\n<p>Note that point prompts and box prompts can be combined. When you're using point prompts you can only segment one object at a time. With box prompts you can segment several objects at once.</p>\n\n<p>Check out <a href=\"https://youtu.be/DfWE_XRcqN8\">this video</a> for an example of how to use the interactive 2d annotator.</p>\n\n<p>We also provide the <code>image series annotator</code>, which can be used for running the 2d annotator for several images in a folder. You can start by clicking <code>Image series annotator</code> in the GUI, running <code>micro_sam.image_series_annotator</code> in the command line or from a <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/image_series_annotator.py\">python script</a>.</p>\n\n<h2 id=\"annotator-3d\">Annotator 3D</h2>\n\n<p>The 3d annotator can be started by</p>\n\n<ul>\n<li>clicking <code>3d annotator</code> in the <code>micro_sam</code> GUI.</li>\n<li>running <code>$ micro_sam.annotator_3d</code> in the command line. Run <code>micro_sam.annotator_3d -h</code> for details.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_3d</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_3d.py\">examples/annotator_3d.py</a> for details.</li>\n</ul>\n\n<p>The user interface of the 3d annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/3d-annotator-menu.png\" width=\"768\"></p>\n\n<p>Most elements are the same as in <a href=\"#annotator-2d\">the 2d annotator</a>:</p>\n\n<ol>\n<li>The napari layers that contain the image, segmentation and prompts. Same as for <a href=\"#annotator-2d\">the 2d annotator</a> but without the <code>auto_segmentation</code> layer.</li>\n<li>The prompt menu.</li>\n<li>The menu for interactive segmentation.</li>\n<li>The 3d segmentation menu. Pressing <code>Segment Volume</code> (or <code>v</code>) will extend the segmentation for the current object across the volume.</li>\n<li>The menu for committing the segmentation.</li>\n<li>The menu for clearing the current annotations.</li>\n</ol>\n\n<p>Note that you can only segment one object at a time with the 3d annotator.</p>\n\n<p>Check out <a href=\"https://youtu.be/5Jo_CtIefTM\">this video</a> for an overview of the interactive 3d segmentation functionality.</p>\n\n<h2 id=\"annotator-tracking\">Annotator Tracking</h2>\n\n<p>The tracking annotator can be started by</p>\n\n<ul>\n<li>clicking <code>Tracking annotator</code> in the <code>micro_sam</code> GUI.</li>\n<li>running <code>$ micro_sam.annotator_tracking</code> in the command line. Run <code>micro_sam.annotator_tracking -h</code> for details.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_tracking</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_tracking.py\">examples/annotator_tracking.py</a> for details. </li>\n</ul>\n\n<p>The user interface of the tracking annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/tracking-annotator-menu.png\" width=\"768\"></p>\n\n<p>Most elements are the same as in <a href=\"#annotator-2d\">the 2d annotator</a>:</p>\n\n<ol>\n<li>The napari layers that contain the image, segmentation and prompts. Same as for <a href=\"#annotator-2d\">the 2d segmentation app</a> but without the <code>auto_segmentation</code> layer, <code>current_tracks</code> and <code>committed_tracks</code> are the equivalent of <code>current_object</code> and <code>committed_objects</code>.</li>\n<li>The prompt menu.</li>\n<li>The menu with tracking settings: <code>track_state</code> is used to indicate that the object you are tracking is dividing in the current frame. <code>track_id</code> is used to select which of the tracks after division you are following.</li>\n<li>The menu for interactive segmentation.</li>\n<li>The tracking menu. Press <code>Track Object</code> (or <code>v</code>) to track the current object across time.</li>\n<li>The menu for committing the current tracking result.</li>\n<li>The menu for clearing the current annotations.</li>\n</ol>\n\n<p>Note that the tracking annotator only supports 2d image data, volumetric data is not supported.</p>\n\n<p>Check out <a href=\"https://youtu.be/PBPW0rDOn9w\">this video</a> for an overview of the interactive tracking functionality.</p>\n\n<h2 id=\"tips-tricks\">Tips &amp; Tricks</h2>\n\n<ul>\n<li>Segment Anything was trained with a fixed image size of 1024 x 1024 pixels. Inputs that do not match this size will be internally resized to match it. Hence, applying Segment Anything to a much larger image will often lead to inferior results, because it will be downsampled by a large factor and the objects in the image become too small.\nTo address this image we implement tiling: cutting up the input image into tiles of a fixed size (with a fixed overlap) and running Segment Anything for the individual tiles.\nYou can activate tiling by passing the parameters <code>tile_shape</code>, which determines the size of the inner tile and <code>halo</code>, which determines the size of the additional overlap.\n<ul>\n<li>If you're using the <code>micro_sam</code> GUI you can specify the values for the <code>halo</code> and <code>tile_shape</code> via the <code>Tile X</code>, <code>Tile Y</code>, <code>Halo X</code> and <code>Halo Y</code>.</li>\n<li>If you're using a python script you can pass them as tuples, e.g. <code>tile_shape=(1024, 1024), halo=(128, 128)</code>. See also <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/0921581e2964139194d235a87cb002d3f3667f45/examples/annotator_2d.py#L40\">the wholeslide_annotator example</a>.</li>\n<li>If you're using the command line functions you can pass them via the options <code>--tile_shape 1024 1024 --halo 128 128</code></li>\n<li>Note that prediction with tiling only works when the embeddings are cached to file, so you must specify an <code>embedding_path</code> (<code>-e</code> in the CLI).</li>\n<li>You should choose the <code>halo</code> such that it is larger than half of the maximal radius of the objects your segmenting.</li>\n</ul></li>\n<li>The applications pre-compute the image embeddings produced by SegmentAnything and (optionally) store them on disc. If you are using a CPU this step can take a while for 3d data or timeseries (you will see a progress bar with a time estimate). If you have access to a GPU without graphical interface (e.g. via a local computer cluster or a cloud provider), you can also pre-compute the embeddings there and then copy them to your laptop / local machine to speed this up. You can use the command <code>micro_sam.precompute_embeddings</code> for this (it is installed with the rest of the applications). You can specify the location of the precomputed embeddings via the <code>embedding_path</code> argument.</li>\n<li>Most other processing steps are very fast even on a CPU, so interactive annotation is possible. An exception is the automatic segmentation step (2d segmentation), which takes several minutes without a GPU (depending on the image size). For large volumes and timeseries segmenting an object in 3d / tracking across time can take a couple settings with a CPU (it is very fast with a GPU).</li>\n<li>You can also try using a smaller version of the SegmentAnything model to speed up the computations. For this you can pass the <code>model_type</code> argument and either set it to <code>vit_b</code> or to <code>vit_l</code> (default is <code>vit_h</code>). However, this may lead to worse results.</li>\n<li>You can save and load the results from the <code>committed_objects</code> / <code>committed_tracks</code> layer to correct segmentations you obtained from another tool (e.g. CellPose) or to save intermediate annotation results. The results can be saved via <code>File -&gt; Save Selected Layer(s) ...</code> in the napari menu (see the tutorial videos for details). They can be loaded again by specifying the corresponding location via the <code>segmentation_result</code> (2d and 3d segmentation) or <code>tracking_result</code> (tracking) argument.</li>\n</ul>\n\n<h2 id=\"known-limitations\">Known limitations</h2>\n\n<ul>\n<li>Segment Anything does not work well for very small or fine-grained objects (e.g. filaments).</li>\n<li>For the automatic segmentation functionality we currently rely on the automatic mask generation provided by SegmentAnything. It is slow and often misses objects in microscopy images. For now, we only offer this functionality in the 2d segmentation app; we are working on improving it and extending it to 3d segmentation and tracking.</li>\n<li>Prompt bounding boxes do not provide the full functionality for tracking yet (they cannot be used for divisions or for starting new tracks). See also <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/23\">this github issue</a>.</li>\n</ul>\n\n<h1 id=\"how-to-use-the-python-library\">How to use the Python Library</h1>\n\n<p>The python library can be imported via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam</span>\n</code></pre>\n</div>\n\n<p>It implements functionality for running Segment Anything for 2d and 3d data, provides more instance segmentation functionality and several other helpful functions for using Segment Anything.\nThis functionality is used to implement the <code>micro_sam</code> annotation tools, but you can also use it as a standalone python library. Check out the documentation under <code>Submodules</code> for more details on the python library.</p>\n\n<h2 id=\"finetuned-models\">Finetuned models</h2>\n\n<p>We provide fine-tuned Segment Anything models for microscopy data. They are still in an experimental stage and we will upload more and better models soon, as well as the code for fine-tuning.\nFor using the current models, check out the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py#L62\">2d annotator example</a> and set <code>use_finetuned_model</code> to <code>True</code>.\nSee the difference between the normal and fine-tuned Segment Anything ViT-h model on an image from <a href=\"https://sartorius-research.github.io/LIVECell/\">LiveCELL</a>:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/vanilla-v-finetuned.png\" width=\"768\"></p>\n"}, {"fullname": "micro_sam.instance_segmentation", "modulename": "micro_sam.instance_segmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.mask_data_to_segmentation", "modulename": "micro_sam.instance_segmentation", "qualname": "mask_data_to_segmentation", "kind": "function", "doc": "<p>Convert the output of the automatic mask generation to an instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>masks:</strong>  The outputs generated by AutomaticMaskGenerator or EmbeddingMaskGenerator.\nOnly supports output_mode=binary_mask.</li>\n<li><strong>shape:</strong>  The image shape.</li>\n<li><strong>with_background:</strong>  Whether the segmentation has background. If yes this function assures that the largest\nobject in the output will be mapped to zero (the background value).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">masks</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">shape</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase", "kind": "class", "doc": "<p>Base class for the automatic mask generators.</p>\n", "bases": "abc.ABC"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.is_initialized", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.is_initialized", "kind": "variable", "doc": "<p>Whether the mask generator has already been initialized.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.crop_list", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.crop_list", "kind": "variable", "doc": "<p>The list of mask data after initialization.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.crop_boxes", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.crop_boxes", "kind": "variable", "doc": "<p>The list of crop boxes.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.original_size", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.original_size", "kind": "variable", "doc": "<p>The original image size.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.get_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.get_state", "kind": "function", "doc": "<p>Get the initialized state of the mask generator.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>State of the mask generator.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.set_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.set_state", "kind": "function", "doc": "<p>Set the state of the mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state:</strong>  The state of the mask generator, e.g. from serialized state.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a point grid.</p>\n\n<p>This class implements the same logic as\n<a href=\"https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py\">https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py</a>\nIt decouples the computationally expensive steps of generating masks from the cheap post-processing operation\nto filter these masks to enable grid search and interactively changing the post-processing.</p>\n\n<p>Use this class as follows:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">amg</span> <span class=\"o\">=</span> <span class=\"n\">AutomaticMaskGenerator</span><span class=\"p\">(</span><span class=\"n\">predictor</span><span class=\"p\">)</span>\n<span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>  <span class=\"c1\"># Initialize the masks, this takes care of all expensive computations.</span>\n<span class=\"n\">masks</span> <span class=\"o\">=</span> <span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">pred_iou_thresh</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate the masks. This is fast and enables testing parameters</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points_per_side:</strong>  The number of points to be sampled along one side of the image.\nIf None, <code>point_grids</code> must provide explicit point sampling.</li>\n<li><strong>points_per_batch:</strong>  The number of points run simultaneously by the model.\nHigher numbers may be faster but use more GPU memory.</li>\n<li><strong>crop_n_layers:</strong>  If &gt;0, the mask prediction will be run again on crops of the image.</li>\n<li><strong>crop_overlap_ratio:</strong>  Sets the degree to which crops overlap.</li>\n<li><strong>crop_n_points_downscale_factor:</strong>  How the number of points is downsampled when predicting with crops.</li>\n<li><strong>point_grids:</strong>  A lisst over explicit grids of points used for sampling masks.\nNormalized to [0, 1] with respect to the image coordinate system.</li>\n</ul>\n", "bases": "AMGBase"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_side</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_batch</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">crop_n_layers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">crop_overlap_ratio</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.3413333333333333</span>,</span><span class=\"param\">\t<span class=\"n\">crop_n_points_downscale_factor</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">point_grids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.generate", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred_iou_thresh:</strong>  Filter threshold in [0, 1], using the mask quality predicted by the model.</li>\n<li><strong>stability_score_thresh:</strong>  Filter threshold in [0, 1], using the stability of the mask\nunder changes to the cutoff used to binarize the model prediction.</li>\n<li><strong>stability_score_offset:</strong>  The amount to shift the cutoff when calculating the stability score.</li>\n<li><strong>box_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks.</li>\n<li><strong>crop_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks between crops.</li>\n<li><strong>min_mask_region_area:</strong>  Minimal size for the predicted masks.</li>\n<li><strong>output_mode:</strong>  The form masks are returned in.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">pred_iou_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.88</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.95</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_offset</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">box_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">crop_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">min_mask_region_area</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;binary_mask&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.EmbeddingMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "EmbeddingMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using an initial segmentations derived from image embeddings.</p>\n\n<p>Uses an intial segmentation derived from the image embeddings via the Mutex Watershed,\nan affinity based sementation method.\nThe computationally expensive steps of the mask generation are decoupled from cheaper post-processing operations,\nto enable faster grid search and interactively changing the post-processing.</p>\n\n<p>Use this class as follows:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">amg</span> <span class=\"o\">=</span> <span class=\"n\">EmbeddingMaskGenerator</span><span class=\"p\">(</span><span class=\"n\">predictor</span><span class=\"p\">)</span>\n<span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>  <span class=\"c1\"># Initialize the masks, this takes care of all expensive computations.</span>\n<span class=\"n\">masks</span> <span class=\"o\">=</span> <span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">pred_iou_thresh</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate the masks. This is fast and enables testing parameters</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>offsets:</strong>  Offset values for the affinities computed from image embeddings that are used\nfor the mutex watershed.</li>\n<li><strong>min_initial_size:</strong>  Minimal size of initial segments.</li>\n<li><strong>distance_type:</strong>  The distance function used to turn embeddings into affinities.</li>\n<li><strong>bias:</strong>  Value to bias the initial segmentation towards over-segmentation.</li>\n<li><strong>use_box:</strong>  Whether to use boxes derived from the initial segments as prompts.</li>\n<li><strong>use_mask:</strong>  Whether to use the initial segments as prompts.</li>\n<li><strong>use_points:</strong>  Whether to use points derived from the initial segments as prompts.</li>\n<li><strong>box_extension:</strong>  Factor for extending the bounding box prompts, given in the relative box size.</li>\n</ul>\n", "bases": "AMGBase"}, {"fullname": "micro_sam.instance_segmentation.EmbeddingMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "EmbeddingMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_initial_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">distance_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;l2&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">use_box</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.EmbeddingMaskGenerator.default_offsets", "modulename": "micro_sam.instance_segmentation", "qualname": "EmbeddingMaskGenerator.default_offsets", "kind": "variable", "doc": "<p></p>\n", "default_value": "[[-1, 0], [0, -1], [-3, 0], [0, -3], [-9, 0], [0, -9]]"}, {"fullname": "micro_sam.instance_segmentation.EmbeddingMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "EmbeddingMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if 'image' has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.EmbeddingMaskGenerator.generate", "modulename": "micro_sam.instance_segmentation", "qualname": "EmbeddingMaskGenerator.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred_iou_thresh:</strong>  Filter threshold in [0, 1], using the mask quality predicted by the model.</li>\n<li><strong>stability_score_thresh:</strong>  Filter threshold in [0, 1], using the stability of the mask\nunder changes to the cutoff used to binarize the model prediction.</li>\n<li><strong>stability_score_offset:</strong>  The amount to shift the cutoff when calculating the stability score.</li>\n<li><strong>box_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks.</li>\n<li><strong>min_mask_region_area:</strong>  Minimal size for the predicted masks.</li>\n<li><strong>output_mode:</strong>  The form masks are returned in.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">pred_iou_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.88</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.95</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_offset</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">box_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">min_mask_region_area</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;binary_mask&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.EmbeddingMaskGenerator.get_initial_segmentation", "modulename": "micro_sam.instance_segmentation", "qualname": "EmbeddingMaskGenerator.get_initial_segmentation", "kind": "function", "doc": "<p>Get the initial instance segmentation.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The initial instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.EmbeddingMaskGenerator.get_state", "modulename": "micro_sam.instance_segmentation", "qualname": "EmbeddingMaskGenerator.get_state", "kind": "function", "doc": "<p>Get the initialized state of the mask generator.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>State of the mask generator.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.EmbeddingMaskGenerator.set_state", "modulename": "micro_sam.instance_segmentation", "qualname": "EmbeddingMaskGenerator.set_state", "kind": "function", "doc": "<p>Set the state of the mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state:</strong>  The state of the mask generator, e.g. from serialized state.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a point grid.</p>\n\n<p>Implements the same functionality as <code>AutomaticMaskGenerator</code> but for tiled embeddings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points_per_side:</strong>  The number of points to be sampled along one side of the image.\nIf None, <code>point_grids</code> must provide explicit point sampling.</li>\n<li><strong>points_per_batch:</strong>  The number of points run simultaneously by the model.\nHigher numbers may be faster but use more GPU memory.</li>\n<li><strong>point_grids:</strong>  A lisst over explicit grids of points used for sampling masks.\nNormalized to [0, 1] with respect to the image coordinate system.</li>\n</ul>\n", "bases": "AutomaticMaskGenerator"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_side</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_batch</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">point_grids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>tile_shape:</strong>  The tile shape for embedding prediction.</li>\n<li><strong>halo:</strong>  The overlap of between tiles.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n<li><strong>embedding_save_path:</strong>  Where to save the image embeddings.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_save_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledEmbeddingMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledEmbeddingMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using an initial segmentations derived from image embeddings.</p>\n\n<p>Implements the same logic as <code>EmbeddingMaskGenerator</code>, but for tiled image embeddings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>n_threads:</strong>  The number of threads used for parallelize operations over the tiles.</li>\n<li><strong>with_background:</strong>  Whether to run segmentation with background.</li>\n<li><strong>**kwargs:</strong>  Keywoard arguments for <code>EmbeddingMaskGenerator</code>.</li>\n</ul>\n", "bases": "EmbeddingMaskGenerator"}, {"fullname": "micro_sam.instance_segmentation.TiledEmbeddingMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledEmbeddingMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">n_threads</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.TiledEmbeddingMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledEmbeddingMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if 'image' has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>tile_shape:</strong>  The tile shape for embedding prediction.</li>\n<li><strong>halo:</strong>  The overlap of between tiles.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n<li><strong>embedding_save_path:</strong>  Where to save the image embeddings.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_save_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledEmbeddingMaskGenerator.generate", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledEmbeddingMaskGenerator.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred_iou_thresh:</strong>  Filter threshold in [0, 1], using the mask quality predicted by the model.</li>\n<li><strong>stability_score_thresh:</strong>  Filter threshold in [0, 1], using the stability of the mask\nunder changes to the cutoff used to binarize the model prediction.</li>\n<li><strong>stability_score_offset:</strong>  The amount to shift the cutoff when calculating the stability score.</li>\n<li><strong>box_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks.</li>\n<li><strong>min_mask_region_area:</strong>  Minimal size for the predicted masks.</li>\n<li><strong>verbose:</strong>  Whether to print progress of the computation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">pred_iou_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.88</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.95</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_offset</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">box_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">min_mask_region_area</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledEmbeddingMaskGenerator.get_initial_segmentation", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledEmbeddingMaskGenerator.get_initial_segmentation", "kind": "function", "doc": "<p>Get the initial instance segmentation.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The initial instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledEmbeddingMaskGenerator.get_state", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledEmbeddingMaskGenerator.get_state", "kind": "function", "doc": "<p>Get the initialized state of the mask generator.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>State of the mask generator.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledEmbeddingMaskGenerator.set_state", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledEmbeddingMaskGenerator.set_state", "kind": "function", "doc": "<p>Set the state of the mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state:</strong>  The state of the mask generator, e.g. from serialized state.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation", "modulename": "micro_sam.prompt_based_segmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_points", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_points", "kind": "function", "doc": "<p>Segmentation from point prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points:</strong>  The point prompts given in the image coordinate system.</li>\n<li><strong>labels:</strong>  The labels (positive or negative) associated with the points.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_mask", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_mask", "kind": "function", "doc": "<p>Segmentation from a mask prompt.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>mask:</strong>  The mask used to derive prompts.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>use_box:</strong>  Whether to derive the bounding box prompt from the mask.</li>\n<li><strong>use_mask:</strong>  Whether to use the mask itself as prompt.</li>\n<li><strong>use_points:</strong>  Wehter to derive point prompts from the mask.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>box_extension:</strong>  Relative factor used to enlarge the bounding box prompt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_box</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">original_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_logits</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_box", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_box", "kind": "function", "doc": "<p>Segmentation from a box prompt.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>box:</strong>  The box prompt.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>original_size:</strong>  The original image shape.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">original_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_box_and_points", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_box_and_points", "kind": "function", "doc": "<p>Segmentation from a box prompt and point prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>box:</strong>  The box prompt.</li>\n<li><strong>points:</strong>  The point prompts, given in the image coordinates system.</li>\n<li><strong>labels:</strong>  The point labels, either positive or negative.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>original_size:</strong>  The original image shape.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">original_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_generators", "modulename": "micro_sam.prompt_generators", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator", "kind": "class", "doc": "<p>Generate point and/or box prompts from an instance segmentation.</p>\n\n<p>You can use this class to derive prompts from an instance segmentation, either for\nevaluation purposes or for training Segment Anything on custom data.\nIn order to use this generator you need to precompute the bounding boxes and center\ncoordiantes of the instance segmentation, using e.g. <code>util.get_centers_and_bounding_boxes</code>.\nHere's an example for how to use this class:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># Initialize generator for 1 positive and 4 negative point prompts.</span>\n<span class=\"n\">prompt_generator</span> <span class=\"o\">=</span> <span class=\"n\">PointAndBoxPromptGenerator</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">dilation_strength</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n<span class=\"c1\"># Precompute the bounding boxes for the given segmentation</span>\n<span class=\"n\">bounding_boxes</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">get_centers_and_bounding_boxes</span><span class=\"p\">(</span><span class=\"n\">segmentation</span><span class=\"p\">)</span>\n<span class=\"c1\"># generate point prompts for the object with id 1 in &#39;segmentation&#39;</span>\n<span class=\"n\">seg_id</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"n\">points</span><span class=\"p\">,</span> <span class=\"n\">point_labels</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">prompt_generator</span><span class=\"p\">(</span><span class=\"n\">segmentation</span><span class=\"p\">,</span> <span class=\"n\">seg_id</span><span class=\"p\">,</span> <span class=\"n\">bounding_boxes</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>n_positive_points:</strong>  The number of positive point prompts to generate per mask.</li>\n<li><strong>n_negative_points:</strong>  The number of negative point prompts to generate per mask.</li>\n<li><strong>dilation_strength:</strong>  The factor by which the mask is dilated before generating prompts.</li>\n<li><strong>get_point_prompts:</strong>  Whether to generate point prompts.</li>\n<li><strong>get_box_prompts:</strong>  Whether to generate box prompts.</li>\n</ul>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.__init__", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">n_positive_points</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_negative_points</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dilation_strength</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">get_point_prompts</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_box_prompts</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.n_positive_points", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.n_positive_points", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.n_negative_points", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.n_negative_points", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.dilation_strength", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.dilation_strength", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.get_box_prompts", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.get_box_prompts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.get_point_prompts", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.get_point_prompts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sample_data", "modulename": "micro_sam.sample_data", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sample_data.fetch_image_series_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_image_series_example_data", "kind": "function", "doc": "<p>Download the sample images for the image series annotator.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_wholeslide_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_wholeslide_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads part of a whole-slide image from the NeurIPS Cell Segmentation Challenge.\nSee <a href=\"https://neurips22-cellseg.grand-challenge.org/\">https://neurips22-cellseg.grand-challenge.org/</a> for details on the data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_livecell_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_livecell_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads a single image from the LiveCELL dataset.\nSee <a href=\"https://doi.org/10.1038/s41592-021-01249-6\">https://doi.org/10.1038/s41592-021-01249-6</a> for details on the data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_hela_2d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_hela_2d_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads a single image from the HeLa CTC dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_3d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_3d_example_data", "kind": "function", "doc": "<p>Download the sample data for the 3d annotator.</p>\n\n<p>This downloads the Lucchi++ datasets from <a href=\"https://casser.io/connectomics/\">https://casser.io/connectomics/</a>.\nIt is a dataset for mitochondria segmentation in EM.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_tracking_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_tracking_example_data", "kind": "function", "doc": "<p>Download the sample data for the tracking annotator.</p>\n\n<p>This data is the cell tracking challenge dataset DIC-C2DH-HeLa.\nCell tracking challenge webpage: <a href=\"http://data.celltrackingchallenge.net\">http://data.celltrackingchallenge.net</a>\nHeLa cells on a flat glass\nDr. G. van Cappellen. Erasmus Medical Center, Rotterdam, The Netherlands\nTraining dataset: <a href=\"http://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip\">http://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip</a> (37 MB)\nChallenge dataset: <a href=\"http://data.celltrackingchallenge.net/challenge-datasets/DIC-C2DH-HeLa.zip\">http://data.celltrackingchallenge.net/challenge-datasets/DIC-C2DH-HeLa.zip</a> (41 MB)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util", "modulename": "micro_sam.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.util.ImageEmbeddings", "modulename": "micro_sam.util", "qualname": "ImageEmbeddings", "kind": "variable", "doc": "<p></p>\n", "default_value": "typing.Dict[str, typing.Any]"}, {"fullname": "micro_sam.util.get_sam_model", "modulename": "micro_sam.util", "qualname": "get_sam_model", "kind": "function", "doc": "<p>Get the SegmentAnything Predictor.</p>\n\n<p>This function will download the required model checkpoint or load it from file if it\nwas already downloaded. By default the models are downloaded to '~/.sam_models'.\nThis location can be changed by setting the environment variable SAM_MODELS.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>device:</strong>  The device for the model. If none is given will use GPU if available.</li>\n<li><strong>model_type:</strong>  The SegmentAnything model to use.</li>\n<li><strong>checkpoint_path:</strong>  The path to the corresponding checkpoint if not in the default model folder.</li>\n<li><strong>return_sam:</strong>  Return the sam model object as well as the predictor.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segment anything predictor.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_h&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_sam</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.precompute_image_embeddings", "modulename": "micro_sam.util", "qualname": "precompute_image_embeddings", "kind": "function", "doc": "<p>Compute the image embeddings (output of the encoder) for the input.</p>\n\n<p>If 'save_path' is given the embeddings will be loaded/saved in a zarr container.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor</li>\n<li><strong>input_:</strong>  The input data. Can be 2 or 3 dimensional, corresponding to an image, volume or timeseries.</li>\n<li><strong>save_path:</strong>  Path to save the embeddings in a zarr container.</li>\n<li><strong>lazy_loading:</strong>  Whether to load all embeddings into memory or return an\nobject to load them on demand when required. This only has an effect if 'save_path' is given\nand if the input is 3 dimensional.</li>\n<li><strong>ndim:</strong>  The dimensionality of the data. If not given will be deduced from the input data.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled prediction. By default prediction is run without tiling.</li>\n<li><strong>halo:</strong>  Overlap of the tiles for tiled prediction.</li>\n<li><strong>wrong_file_callback [callable]:</strong>  Function to call when an embedding file with wrong file signature\nis passed. If none is given a wrong file signature will cause a warning.\nThe callback ,ust have the signature 'def callback(save_path: str) -> str',\nwhere the return value is the (potentially updated) embedding save path.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">input_</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lazy_loading</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">wrong_file_callback</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.set_precomputed", "modulename": "micro_sam.util", "qualname": "set_precomputed", "kind": "function", "doc": "<p>Set the precomputed image embeddings for a predictor.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_embeddings:</strong>  The precomputed image embeddings computed by <code>precompute_image_embeddings</code>.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.compute_iou", "modulename": "micro_sam.util", "qualname": "compute_iou", "kind": "function", "doc": "<p>Compute the intersection over union of two masks.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask1:</strong>  The first mask.</li>\n<li><strong>mask2:</strong>  The second mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The intersection over union of the two masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">mask1</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>, </span><span class=\"param\"><span class=\"n\">mask2</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_centers_and_bounding_boxes", "modulename": "micro_sam.util", "qualname": "get_centers_and_bounding_boxes", "kind": "function", "doc": "<p>Returns the center coordinates of the foreground instances in the ground-truth.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The segmentation.</li>\n<li><strong>mode:</strong>  Determines the functionality used for computing the centers.</li>\n<li>If 'v', the object's eccentricity centers computed by vigra are used.</li>\n<li>If 'p' the object's centroids computed by skimage are used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dictionary that maps object ids to the corresponding centroid.\n  A dictionary that maps object_ids to the corresponding bounding box.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;v&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.load_image_data", "modulename": "micro_sam.util", "qualname": "load_image_data", "kind": "function", "doc": "<p>Helper function to load image data from file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  The filepath to the image data.</li>\n<li><strong>key:</strong>  The internal filepath for complex data formats like hdf5.</li>\n<li><strong>lazy_loading:</strong>  Whether to lazyly load data. Only supported for n5 and zarr data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The image data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lazy_loading</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.visualization", "modulename": "micro_sam.visualization", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.visualization.compute_pca", "modulename": "micro_sam.visualization", "qualname": "compute_pca", "kind": "function", "doc": "<p>Compute the pca projection of the embeddings to visualize them as RGB image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>embeddings:</strong>  The embeddings. For example predicted by the SAM image encoder.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>PCA of the embeddings, mapped to the pixels.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embeddings</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.visualization.project_embeddings_for_visualization", "modulename": "micro_sam.visualization", "qualname": "project_embeddings_for_visualization", "kind": "function", "doc": "<p>Project image embeddings to pixel-wise PCA.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The PCA of the embeddings.\n  The scale factor for resizing to the original image size.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();